# Loop-It Kubernetes Setup

Ein vollstÃ¤ndiges production-ready Kubernetes-Setup fÃ¼r die Loop-It Anwendung mit Docker Desktop, NGINX Ingress Controller, Horizontal Pod Autoscaling und automatisiertem Deployment.

## ğŸš€ Quick Start

```bash
# 1. Repository clonen
git clone https://github.com/Loop-It-Project/Loop-It
cd Loop-It

# 2. Docker Desktop mit Kubernetes starten
# Settings â†’ Kubernetes â†’ Enable Kubernetes

# 3. Loop-It mit Production-Features deployen
./k8s/deploy.sh

# 4. Im Browser Ã¶ffnen
open http://localhost
```

## ğŸ“‹ Voraussetzungen

- **Docker Desktop** mit aktiviertem Kubernetes
- **Git Bash** oder Terminal
- **curl** fÃ¼r API-Tests
- **Mindestens 6GB RAM** fÃ¼r alle Services inkl. Auto-Scaling
- **Artillery** fÃ¼r Load Testing (optional): `npm install -g artillery`

### Docker Desktop Setup
1. Docker Desktop installieren
2. Settings â†’ Kubernetes â†’ "Enable Kubernetes" aktivieren
3. Warten bis Kubernetes lÃ¤uft (grÃ¼ner Punkt)

## ğŸ—ï¸ Production-Ready Architektur

```
Internet â†’ localhost:80
    â†“
NGINX Ingress Controller (Load Balancer)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kubernetes Cluster                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Frontend      â”‚       Backend           â”‚  â”‚
â”‚  â”‚   (React/Vite)  â”‚      (Node.js)          â”‚  â”‚
â”‚  â”‚   2-10 Pods     â”‚      2-20 Pods          â”‚  â”‚
â”‚  â”‚   Auto-Scaling  â”‚      Auto-Scaling       â”‚  â”‚
â”‚  â”‚   Port 80       â”‚      Port 3000          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†“                                  â”‚
â”‚         PostgreSQL                              â”‚
â”‚         Port 5432                               â”‚
â”‚         (Persistent Storage + PDB)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Production Services
- **Frontend**: React/Vite App (NGINX Container) mit HPA
- **Backend**: Node.js API Server mit HPA und Health Checks
- **PostgreSQL**: Datenbank mit persistentem Storage und Pod Disruption Budget
- **NGINX Ingress**: Production Load Balancer & Reverse Proxy
- **HPA**: Horizontal Pod Autoscaler fÃ¼r automatische Skalierung
- **PDB**: Pod Disruption Budgets fÃ¼r Zero-Downtime Updates

## ğŸš€ Production Features

### Horizontal Pod Autoscaling (HPA)
- **Backend**: Skaliert von 2-20 Pods basierend auf CPU (70%) und Memory (80%)
- **Frontend**: Skaliert von 2-10 Pods basierend auf CPU (60%) und Memory (70%)
- **Automatische Load-Verteilung** bei Traffic-Spitzen
- **Validiert mit Artillery Load Testing** bis 622 RPS

### Health Checks & Self-Healing
- **Startup Probes**: Optimierte Container-Startzeiten (60s statt 120s)
- **Liveness Probes**: Automatische Container-Restarts bei Problemen
- **Readiness Probes**: Traffic-Kontrolle fÃ¼r healthy Pods
- **Graceful Shutdown**: 30s Termination Grace Period

### Zero-Downtime Operations
- **Pod Disruption Budgets**: Garantierte 50% Backend-VerfÃ¼gbarkeit wÃ¤hrend Updates
- **Rolling Updates**: Sanfte Deployments ohne Service-Unterbrechung
- **Load Balancer Integration**: Automatische healthy Pod-Erkennung

## ğŸ—ƒï¸ Datenpersistenz & Storage

Die PostgreSQL-Datenbank speichert ihre Daten **persistent** mit einem `PersistentVolumeClaim`. Dadurch bleiben alle Nutzerdaten auch nach Pod-Restarts erhalten.

### ğŸ” Speicherort der Daten
```bash
Container: /var/lib/postgresql/data
PVC: postgres-pvc (1Gi, ReadWriteOnce)
```

### ğŸ“¦ Storage-Konfiguration
```yaml
# Automatisch erstellt beim Deployment
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: loopit-dev
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

## ğŸ”§ Production Deployment

### Automatisches Deployment
```bash
# Production-ready Deployment mit allen Features
./k8s/deploy.sh
```

**Was passiert beim Deployment:**
1. âœ… NGINX Ingress Controller v1.13.0 installieren
2. ğŸ”¨ Docker Images bauen (Backend + Frontend)
3. ğŸ§¹ Clean Deployment (alte Pods entfernen)
4. ğŸ“¦ Namespace + Secrets erstellen
5. ğŸ—„ï¸ PostgreSQL mit Persistent Storage deployen
6. ğŸš€ Backend mit HPA + Health Checks deployen
7. ğŸ¨ Frontend mit HPA deployen
8. ğŸŒ NGINX Ingress konfigurieren
9. ğŸ“Š Production-Status anzeigen

### Erwartete Deployment-Zeit
- **Clean Deployment**: ~3-5 Minuten
- **Backend Startup**: ~60 Sekunden (optimiert)
- **HPA Aktivierung**: ~30 Sekunden
- **Ingress Ready**: ~30 Sekunden

## ğŸŒ Zugriff & APIs

| Service | URL | Beschreibung |
|---------|-----|--------------|
| Frontend | http://localhost/ | React App |
| Backend Health | http://localhost/api/health | API Health Check |
| Backend Metrics | http://localhost/api/metrics | Prometheus Metrics |
| Backend APIs | http://localhost/api/* | Alle API Endpoints |

### API Beispiele
```bash
# Health Check mit Environment Info
curl http://localhost/api/health
# Response: {"status":"OK","hasJwtSecret":true,"hasDbUrl":true,"port":"3000"}

# Prometheus Metriken fÃ¼r Monitoring
curl http://localhost/api/metrics

# Weitere APIs (abhÃ¤ngig von Backend-Implementation)
curl http://localhost/api/universes/user/owned
curl http://localhost/api/auth/status
```

## ğŸ“Š Production Monitoring & Debugging

### Status prÃ¼fen
```bash
# HPA Auto-Scaling Status
kubectl get hpa -n loopit-dev

# Pod-Status mit Resource-Usage
kubectl get pods -n loopit-dev -o wide

# Pod Disruption Budgets
kubectl get pdb -n loopit-dev

# Service + Ingress Status
kubectl get services,ingress -n loopit-dev

# Storage Status
kubectl get pvc -n loopit-dev
```

### Live Monitoring
```bash
# HPA Scaling live verfolgen
kubectl get hpa -n loopit-dev -w

# Pod-Skalierung beobachten
kubectl get pods -n loopit-dev -w

# Resource Usage
kubectl top pods -n loopit-dev

# Scaling Events
kubectl get events -n loopit-dev --field-selector reason=SuccessfulRescale
```

### Logs & Debugging
```bash
# Application Logs
kubectl logs -l app=backend -n loopit-dev
kubectl logs -l app=frontend -n loopit-dev
kubectl logs -l app=postgres -n loopit-dev

# NGINX Ingress Logs
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx

# Live Log-Verfolgung
kubectl logs -f -l app=backend -n loopit-dev

# Pod-Details bei Problemen
kubectl describe pod <pod-name> -n loopit-dev
```

## ğŸ”¥ Load Testing & Performance

### Artillery Load Testing Setup
```bash
# Artillery installieren
npm install -g artillery

# Load Test Config erstellen
mkdir -p k8s/load-testing
cat > k8s/load-testing/stress-test.yml << 'EOF'
config:
  target: 'http://localhost'
  phases:
    - duration: 60
      arrivalRate: 20
      rampTo: 100
      name: "Heavy Ramp"
    - duration: 120  
      arrivalRate: 150
      name: "Stress Load"

scenarios:
  - name: "Backend Load"
    weight: 60
    flow:
      - get:
          url: "/api/health"
      - get:
          url: "/api/metrics"
  - name: "Frontend Load"
    weight: 40
    flow:
      - get:
          url: "/"
EOF

# Stress Test starten
artillery run k8s/load-testing/stress-test.yml
```

### Validierte Performance
- **Getestet bis 622 RPS** ohne Service-Degradation
- **Auto-Scaling**: 2 â†’ 14 Backend Pods bei 156% CPU Load
- **Recovery**: System normalisiert sich automatisch auf 3% CPU
- **Response Times**: P95 < 10ms unter normaler Last

## ğŸ§¹ Cleanup

```bash
# VollstÃ¤ndiges Cleanup (inkl. PVC-Daten)
./k8s/cleanup.sh

# Oder manuell (Pods + Services)
kubectl delete namespace loopit-dev

# NGINX Ingress Controller behalten
kubectl delete namespace loopit-dev --ignore-not-found
```

**Was wird gelÃ¶scht:**
- Alle Loop-It Services und Pods
- HPA und Pod Disruption Budgets
- Generierte Secrets
- PersistentVolumeClaim (âš ï¸ Daten gehen verloren!)
- Optional: Docker Images und NGINX Ingress Controller

## ğŸ” Production Security

### Secrets Management
Secrets werden automatisch generiert und rotiert:
```bash
# Generierte Secrets
- PostgreSQL Password: 16-stelliger Hex-String
- JWT Secret: 32-stelliger Hex-String  
- JWT Refresh Secret: 32-stelliger Hex-String

# Secrets anzeigen
kubectl get secrets -n loopit-dev

# Secret-Details (Base64 encoded)
kubectl get secret loopit-secrets -n loopit-dev -o yaml
```

### Security Features
- **Non-root Container Users**: Alle Container laufen als unprivileged User
- **Security Contexts**: Dropped Capabilities und eingeschrÃ¤nkte Berechtigungen
- **Network Isolation**: Kubernetes NetworkPolicies-ready
- **Resource Limits**: CPU/Memory Limits gegen Resource Exhaustion
- **Graceful Shutdown**: Verhindert Data Loss bei Pod-Terminierung

## ğŸ”§ Production Configuration

### Environment Variables

**Backend (Production-optimiert):**
```yaml
- PORT: "3000"
- NODE_ENV: "production"
- DB_HOST: "postgres"
- DB_PORT: "5432"
- DATABASE_URL: "postgresql://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@$(DB_HOST):$(DB_PORT)/$(POSTGRES_DB)"
- POSTGRES_USER: <from secret>
- POSTGRES_PASSWORD: <from secret>
- POSTGRES_DB: "loop-it"
- JWT_SECRET: <from secret>
- JWT_REFRESH_SECRET: <from secret>
- JWT_EXPIRES_IN: "7d"
- FRONTEND_URL: "http://localhost"
```

### Production Health Checks

**Backend (optimiert fÃ¼r schnelle Startups):**
```yaml
startupProbe:
  httpGet: {path: /api/health, port: 3000}
  initialDelaySeconds: 10    # Reduziert von 30s
  periodSeconds: 5
  failureThreshold: 20       # 100s total startup time

livenessProbe:
  httpGet: {path: /api/health, port: 3000}
  periodSeconds: 30          # Weniger hÃ¤ufig nach Startup
  failureThreshold: 3

readinessProbe:
  httpGet: {path: /api/health, port: 3000}
  periodSeconds: 5           # Schnelle Traffic-Reaktion
  failureThreshold: 2
```

### Resource Management (Production-tuned)

| Service | Requests | Limits | HPA Range |
|---------|----------|---------|-----------|
| Backend | 200m CPU, 256Mi RAM | 1000m CPU, 512Mi RAM | 2-20 Pods |
| Frontend | 50m CPU, 64Mi RAM | 200m CPU, 128Mi RAM | 2-10 Pods |
| PostgreSQL | 250m CPU, 512Mi RAM | 1000m CPU, 1024Mi RAM | 1 Pod (StatefulSet) |

## ğŸš¨ Production Troubleshooting

### HPA nicht skalierend
```bash
# HPA Status und Metriken prÃ¼fen
kubectl describe hpa backend-hpa -n loopit-dev

# Resource Usage prÃ¼fen
kubectl top pods -n loopit-dev

# Load Test fÃ¼r Skalierung
artillery run k8s/load-testing/stress-test.yml
```

### Pod CrashLoopBackOff
```bash
# HÃ¤ufigste Ursachen bereits behoben:
# âœ… DATABASE_URL fÃ¼r Drizzle ORM
# âœ… Upload-Verzeichnis Volume Mounts
# âœ… NGINX Filesystem-Berechtigungen

# Debugging
kubectl logs <pod-name> -n loopit-dev --previous
kubectl describe pod <pod-name> -n loopit-dev
```

### Ingress nicht erreichbar
```bash
# NGINX Ingress Controller Status
kubectl get pods -n ingress-nginx

# Ingress-Konfiguration prÃ¼fen
kubectl describe ingress loopit-ingress -n loopit-dev

# Manuelle NGINX Ingress Installation (falls nÃ¶tig)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.13.0/deploy/static/provider/cloud/deploy.yaml
```

### Performance-Probleme
```bash
# Resource Bottlenecks identifizieren
kubectl top nodes
kubectl top pods -n loopit-dev

# HPA Scaling-Events prÃ¼fen
kubectl get events -n loopit-dev | grep -i scale

# Load Testing fÃ¼r Performance-Baselines
artillery run k8s/load-testing/stress-test.yml
```

## ğŸ“ Dateistruktur

```
k8s/
â”œâ”€â”€ backend.yaml              # Backend Deployment + Service + HPA + PDB
â”œâ”€â”€ frontend.yaml             # Frontend Deployment + Service + HPA + PDB  
â”œâ”€â”€ postgres.yaml             # PostgreSQL Deployment + Service + PVC + PDB
â”œâ”€â”€ ingress.yaml              # NGINX Ingress Configuration
â”œâ”€â”€ deploy.sh                 # Production Deployment Script
â”œâ”€â”€ cleanup.sh                # Cleanup Script
â”œâ”€â”€ load-testing/             # Artillery Load Testing Configs
â”‚   â”œâ”€â”€ stress-test.yml       # High-Load Test (150 RPS)
â”‚   â””â”€â”€ quick-test.yml        # Basic Load Test (40 RPS)
â””â”€â”€ README.md                 # Diese Dokumentation
```

## ğŸ”„ Production Updates & Rollbacks

### Zero-Downtime Updates
```bash
# Automatisches Rolling Update
./k8s/deploy.sh

# Manuelles Rolling Update einzelner Services
kubectl rollout restart deployment/backend -n loopit-dev
kubectl rollout restart deployment/frontend -n loopit-dev

# Update-Status verfolgen
kubectl rollout status deployment/backend -n loopit-dev
```

### Rollback-Strategien
```bash
# Rollback auf vorherige Version
kubectl rollout undo deployment/backend -n loopit-dev

# Spezifische Revision
kubectl rollout history deployment/backend -n loopit-dev
kubectl rollout undo deployment/backend --to-revision=2 -n loopit-dev

# Rollback-Status prÃ¼fen
kubectl rollout status deployment/backend -n loopit-dev
```

## ğŸš€ Next Steps: Enterprise Features

### Monitoring Stack (empfohlen)
```bash
# Prometheus + Grafana fÃ¼r Custom Metrics HPA
./k8s/monitoring/deploy-monitoring.sh

# Custom Metrics fÃ¼r Request Rate / Response Time basiertes Scaling
# Grafana Dashboards fÃ¼r Application Performance Monitoring
```

### GitOps mit ArgoCD
```bash
# Automatische Deployments Ã¼ber Git
# Declarative Configuration Management
# Multi-Environment Support (Dev/Staging/Prod)
```

### Cloud Migration (AWS EKS)
```bash
# AWS EKS-optimierte Konfigurationen
# ALB Ingress Controller
# EFS/EBS Storage Classes
# IAM Roles for Service Accounts (IRSA)
```

## ğŸ“š WeiterfÃ¼hrende Informationen

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/)
- [Pod Disruption Budgets](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
- [Artillery Load Testing](https://artillery.io/)

---
 
**Auto-Scaling: Validiert bis 622 RPS** ğŸš€  
**Zero-Downtime Updates: Implemented** ğŸ›¡ï¸